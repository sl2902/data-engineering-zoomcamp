{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01a88235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d251cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 15:33:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"local\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097a3e9",
   "metadata": {},
   "source": [
    "### Q1\n",
    "What is the version of Spark in use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f66f2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915fa92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../data/fhvhv_tripdata_2021-06.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "382791ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(path, compression=\"gzip\", nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3a281f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:02:41</td>\n",
       "      <td>2021-06-01 00:07:46</td>\n",
       "      <td>174</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:16:16</td>\n",
       "      <td>2021-06-01 00:21:14</td>\n",
       "      <td>32</td>\n",
       "      <td>254</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:27:01</td>\n",
       "      <td>2021-06-01 00:42:11</td>\n",
       "      <td>240</td>\n",
       "      <td>127</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:46:08</td>\n",
       "      <td>2021-06-01 00:53:45</td>\n",
       "      <td>127</td>\n",
       "      <td>235</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B02510</td>\n",
       "      <td>2021-06-01 00:45:42</td>\n",
       "      <td>2021-06-01 01:03:33</td>\n",
       "      <td>144</td>\n",
       "      <td>146</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropoff_datetime  \\\n",
       "0               B02764  2021-06-01 00:02:41  2021-06-01 00:07:46   \n",
       "1               B02764  2021-06-01 00:16:16  2021-06-01 00:21:14   \n",
       "2               B02764  2021-06-01 00:27:01  2021-06-01 00:42:11   \n",
       "3               B02764  2021-06-01 00:46:08  2021-06-01 00:53:45   \n",
       "4               B02510  2021-06-01 00:45:42  2021-06-01 01:03:33   \n",
       "\n",
       "   PULocationID  DOLocationID SR_Flag Affiliated_base_number  \n",
       "0           174            18       N                 B02764  \n",
       "1            32           254       N                 B02764  \n",
       "2           240           127       N                 B02764  \n",
       "3           127           235       N                 B02764  \n",
       "4           144           146       N                    NaN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cbb5feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num      object\n",
       "pickup_datetime           object\n",
       "dropoff_datetime          object\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "SR_Flag                   object\n",
       "Affiliated_base_number    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "pdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdc307e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "983bb69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9739b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 16:00:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/04 16:00:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/04 16:00:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/04 16:00:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/04 16:00:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/04 16:00:34 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 56.31% for 12 writers\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3a4da",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "Repartition it to 12 partitions and save it to parquet.\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "104f3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a04909f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 16:04:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/04 16:04:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/04 16:04:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/04 16:04:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/04 16:04:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/04 16:04:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 56.31% for 12 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 16:04:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/04 16:04:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/04 16:04:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/04 16:04:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/04 16:04:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:====>                                                    (1 + 11) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".parquet('../data/pq/fhvhv/2021/06/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7c66a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 591360\r\n",
      "-rw-r--r--  1 home  staff     0B Mar  4 16:04 _SUCCESS\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00000-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00001-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00002-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00003-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00004-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00005-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00006-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00007-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00008-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00009-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00010-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 home  staff    24M Mar  4 16:04 part-00011-e8c252af-323e-4185-b32e-90a9e820cea4-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh \"../data/pq/fhvhv/2021/06\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae04e6",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "How many taxi trips were there on June 15?\n",
    "\n",
    "Consider only trips that started on June 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caebb3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2021-06-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a963532",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhvhv_2021_06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad11a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_2021_06\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '2021-06-15';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7fc3b3",
   "metadata": {},
   "source": [
    "### Q4\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "How long was the longest trip in Hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a687392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|pickup_date|max(duration)|\n",
      "+-----------+-------------+\n",
      "| 2021-06-25|       240764|\n",
      "| 2021-06-22|        91979|\n",
      "| 2021-06-27|        71931|\n",
      "| 2021-06-26|        65510|\n",
      "| 2021-06-23|        59281|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration', df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long')) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "    .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0bc1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhvhv_max_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9244e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|      max_duration|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25|  66.8788888888889|\n",
      "| 2021-06-22|25.549722222222222|\n",
      "| 2021-06-27|19.980833333333333|\n",
      "| 2021-06-26|18.197222222222223|\n",
      "| 2021-06-23|16.466944444444444|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) pickup_date,\n",
    "    max((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 3600) max_duration\n",
    "FROM \n",
    "    fhvhv_max_duration\n",
    "GROUP BY\n",
    "    pickup_date\n",
    "ORDER BY\n",
    "    max_duration desc\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0482d95",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "What is the name of the most frequent pickup location zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ffcf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.csv('../data/taxi_zone_lookup.csv', header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19feceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(LocationID='1', Borough='EWR', Zone='Newark Airport', service_zone='EWR'),\n",
       " Row(LocationID='2', Borough='Queens', Zone='Jamaica Bay', service_zone='Boro Zone'),\n",
       " Row(LocationID='3', Borough='Bronx', Zone='Allerton/Pelham Gardens', service_zone='Boro Zone'),\n",
       " Row(LocationID='4', Borough='Manhattan', Zone='Alphabet City', service_zone='Yellow Zone'),\n",
       " Row(LocationID='5', Borough='Staten Island', Zone='Arden Heights', service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "427bf920",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01295979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+------------+\n",
      "|LocationID|Borough|          Zone|service_zone|\n",
      "+----------+-------+--------------+------------+\n",
      "|         1|    EWR|Newark Airport|         EWR|\n",
      "|         2| Queens|   Jamaica Bay|   Boro Zone|\n",
      "+----------+-------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM \n",
    "    zones\n",
    "LIMIT 2;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Zone,\n",
    "    count(*) as max_zones\n",
    "FROM \n",
    "    fhvhv_2021_06 b left join zones a  on a.LocationID = b.PULocationID\n",
    "GROUP BY\n",
    "    Zone\n",
    "ORDER BY\n",
    "    max_zones desc\n",
    "LIMIT 2;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030a192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw5-MPp2380M",
   "language": "python",
   "name": "hw5-mpp2380m"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
